{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train and valid labels...\n",
      "0 lines processed\r",
      "Loading train and valid data...\n",
      "0 lines processed\r"
     ]
    }
   ],
   "source": [
    "import config as cf\n",
    "from util import load_label, load_data\n",
    "\n",
    "print(\"Loading train and valid labels...\")\n",
    "# train_label = load_label(cf.train_label, cf)\n",
    "valid_label = load_label(cf.valid_label, cf)\n",
    "\n",
    "print(\"Loading train and valid data...\")\n",
    "# train_data = load_data(cf.train_data, train_label, True, cf)\n",
    "valid_data = load_data(cf.valid_data, valid_label, False, cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7215\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting text vocab...\n",
      "Collecting time zone vocab...\n",
      "Collecting location vocab...\n",
      "Collecting description vocab...\n",
      "Collecting name vocab...\n",
      "Collecting class labels...\n"
     ]
    }
   ],
   "source": [
    "from util import get_vocab, get_classes\n",
    "\n",
    "train_data = valid_data\n",
    "train_label = valid_label\n",
    "\n",
    "#collect vocab and classes\n",
    "print(\"Collecting text vocab...\")\n",
    "vocabxid, idxvocab, _ = get_vocab(train_data, \"text\", \"char\", cf.word_minfreq)\n",
    "\n",
    "print(\"Collecting time zone vocab...\")\n",
    "tzxid, _, _ = get_vocab(train_data, \"timezone\", \"word\", 0)\n",
    "\n",
    "print(\"Collecting location vocab...\")\n",
    "locxid, _, _ = get_vocab(train_data, \"location\", \"char\", cf.word_minfreq)\n",
    "\n",
    "print(\"Collecting description vocab...\")\n",
    "descxid, _, _ = get_vocab(train_data, \"description\", \"char\", cf.word_minfreq)\n",
    "\n",
    "print(\"Collecting name vocab...\")\n",
    "namexid, _, _ = get_vocab(train_data, \"name\", \"char\", cf.word_minfreq)\n",
    "\n",
    "print(\"Collecting class labels...\")\n",
    "classes = get_classes(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting text to ids...\n",
      "0 instances processed\r"
     ]
    }
   ],
   "source": [
    "from util import clean_data\n",
    "\n",
    "print(\"Converting text to ids...\")\n",
    "valid_len_x, valid_miss_y, valid_len_loc, valid_len_desc, valid_len_name = clean_data(valid_data, valid_label, \\\n",
    "    vocabxid, tzxid, locxid, descxid, namexid, classes, cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting data based on tweet length...\n"
     ]
    }
   ],
   "source": [
    "print(\"Sorting data based on tweet length...\")\n",
    "valid_data = sorted(valid_data, key=lambda item: len(item[\"x\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0905 15:29:08.546841 4642477504 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0905 15:29:08.548924 4642477504 deprecation_wrapper.py:119] From /Users/jinmei/Softwares/twitter-deepgeo/geo_model.py:127: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0905 15:29:08.557450 4642477504 deprecation_wrapper.py:119] From /Users/jinmei/Softwares/twitter-deepgeo/geo_model.py:144: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0905 15:29:08.558362 4642477504 deprecation_wrapper.py:119] From /Users/jinmei/Softwares/twitter-deepgeo/geo_model.py:37: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0905 15:29:08.659535 4642477504 deprecation.py:323] From /Users/jinmei/Softwares/twitter-deepgeo/geo_model.py:59: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v1.nn' has no attribute 'rnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7d27da6ac0e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmtrains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmvalids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mmtrains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTGP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_timezones\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtzxid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc_vsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocxid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0mdesc_vsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescxid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_vsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamexid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#     with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Softwares/twitter-deepgeo/geo_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, is_training, vocab_size, num_steps, num_classes, num_timezones, loc_vsize, desc_vsize, name_vsize, config)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 hidden_text, self.text_attn = self.text_layer(self.x, vocab_size, config.text_emb_size, \\\n\u001b[0;32m--> 146\u001b[0;31m                     config.text_filter_number, num_steps, config.text_pool_window, config)\n\u001b[0m\u001b[1;32m    147\u001b[0m                 \u001b[0mhiddens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Softwares/twitter-deepgeo/geo_model.py\u001b[0m in \u001b[0;36mtext_layer\u001b[0;34m(self, x, vsize, esize, fnum, maxlen, pool_window, config)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lstm-forward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mlstm_fw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mesize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforget_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             fw_outputs, _ = tf.nn.rnn(lstm_fw, inputs_s, \\\n\u001b[0m\u001b[1;32m     61\u001b[0m                 initial_state=lstm_fw.zero_state(config.batch_size, tf.float32))\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m#insert zero state at the front and drop the last state [h_A, h_B, h_C] -> [0, h_A, h_B]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/util/deprecation_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_dw_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accessing local variables before they are created.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dw_wrapped_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     if (self._dw_warning_count < _PER_MODULE_WARNING_LIMIT and\n\u001b[1;32m    108\u001b[0m         name not in self._dw_deprecated_printed):\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v1.nn' has no attribute 'rnn'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from geo_model import TGP\n",
    "\n",
    "#train model\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    tf.set_random_seed(cf.seed)\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    \n",
    "    mtrains, mvalids = [], []\n",
    "    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "        mtrains.append(TGP(is_training=True, vocab_size=len(idxvocab), num_steps=cf.bucket_sizes[0], \\\n",
    "            num_classes=len(classes), num_timezones=len(tzxid), loc_vsize=len(locxid), \\\n",
    "            desc_vsize=len(descxid), name_vsize=len(namexid), config=cf))\n",
    "        \n",
    "#     with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "#         if len(cf.bucket_sizes) > 1:\n",
    "#             for b in cf.bucket_sizes[1:]:\n",
    "#                 mtrains.append(TGP(is_training=True, vocab_size=len(idxvocab), num_steps=b, \\\n",
    "#                     num_classes=len(classes), num_timezones=len(tzxid), loc_vsize=len(locxid), \\\n",
    "#                     desc_vsize=len(descxid), name_vsize=len(namexid),  config=cf))\n",
    "#         for b in cf.bucket_sizes:\n",
    "#             mvalids.append(TGP(is_training=False, vocab_size=len(idxvocab), num_steps=b, \\\n",
    "#                 num_classes=len(classes), num_timezones=len(tzxid), loc_vsize=len(locxid), \\\n",
    "#                 desc_vsize=len(descxid), name_vsize=len(namexid), config=cf))\n",
    "\n",
    "    tf.initialize_all_variables().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
